---
layout: post
title: "Penalized regression using only first-year calculus"
author: Peter Hoff
output: html_document
date:   2017-01-17 
---


<style type="text/css">
body, td { font-size: 16px; }
code.r{ font-size: 14px; }
pre { font-size: 14px }
</style>





### Summary
Modern data analyses  often involve 
too many regressors and not enough observations. In this situation, 
the OLS regression estimator is highly variable and can lead to 
poor predictions. A popular alternative estimator is the Lasso 
regression estimator. In this post I explain how the lasso regression 
estimator may be computed by iterating the following two 
lines of code
```{r,eval=FALSE}
u<- solve( Q * v%*%t(v) + Il/2 )%*%( l*v )

v<- solve( Q * u%*%t(u) + Il/2 )%*%( l*u )  
```
Understanding why this iterative algorithm works requires 
only a basic understanding of linear regression and 
first semester calculus. The full article describing this 
algorithm is detailed in 
[arXiv:1611.00040](https://arxiv.org/abs/1611.00040). Below 
is a summary and example. 

### Penalized linear regression


#### Least squares estimation
If you are familiar with the linear regression model
$$y_i = \beta^\top x_i + \epsilon_i$$ you probably know that
the OLS regression estimate $\hat \beta_{\text{ols}}$ is the minimizer
of the residual sum of squares

$$
 \hat \beta_{\text{ols}} = \arg \min_\beta \sum (y_i - \beta^\top x_i  )^2 . 
$$

It is convenient to rewrite the residual sum of squares as

$$
\begin{align*} 
\sum (y_i - \beta^\top x_i  )^2  &= \sum y_i^2 - 2 \beta^\top \sum y_ix_i +
    \beta^\top \left (\sum x_i x_i^\top \right ) \\
  &= ||y||^2 - 2 \beta^\top l + \beta^\top Q \beta. 
\end{align*}
$$

Since the difference between the RSS at any two 
values of $\beta$ is not affected by the value of $||y||^2$, 
we can write 

$$
 \hat \beta_{\text{ols}} = \arg \min_\beta   \beta^\top Q \beta -  2 \beta^\top l .
$$

Using calculus or orthogonaliy considerations, you can show 
$\hat \beta_{\text{ols}}$ will satisfy 
$2 Q \hat\beta_{\text{ols}} = 2 l$. If 
$Q$ is invertible, then 

$$
\begin{align*}
 \hat \beta_{\text{ols}} & = \arg \min_\beta   \beta^\top Q \beta -  2 \beta^\top l . \\ 
 &= Q^{-1} l.
\end{align*}
$$


#### Ridge regression
As described above, the OLS estimate has fallen out 
of fashion, and people nowadays prefer ~~Bayesian~~ 
penalized estimates given by

$$
\hat \beta = \arg \min_\beta \beta^\top Q \beta -  2 \beta^\top l  + f(\beta)
$$

where $f(\beta)$ is some penalty function. One popular penalty function is 
an $L_2$ or "ridge" penalty, 
given by $f(\beta) = \lambda\beta^\top \beta$ for some $\lambda>0$. 
This penalty can also be written as 
$f(\beta) = \lambda \sum |\beta_j|^2$. 
In this case, the penalized 
regression estimate is given as follows:

$$
\begin{align*}
\hat \beta & = \arg \min_\beta \beta^\top Q \beta -  2 \beta^\top l  +  \lambda \beta^\top \beta   \\ 
   &= \arg \min_\beta \beta^\top (Q + \lambda I) \beta -  2 \beta^\top l.  \\
   &= ( Q + \lambda I  )^{-1} l ,
\end{align*}
$$ 

where the last line follows by using the same logic as used to
obtain the OLS estimator, with $Q$ replaced by $Q+\lambda I$. 
The resulting 
estimator is called a ridge regression estimator, and 
happens to be equal to the posterior mean estimator of
$\beta$ under a $N(0,1/\lambda)$ prior distribution for 
the elements of $\beta$.



#### Lasso regression 
Another popular penalty  on $\beta$ is the $L_1$ or lasso penalty,
given by $f(\beta) =\lambda  \sum |\beta_j|$. A lasso estimate is 
then given by 

$$
\begin{align*}
\hat \beta & = \arg \min_\beta \beta^\top Q \beta -  2 \beta^\top l  +  \lambda \sum | \beta_j|.    
\end{align*}
$$

Unfortunately there is no closed-form expression for the lasso 
estimator. To compute it, you need to do one of the following:

1. Learn convex optimization and then write an algorithm; 
2. Use a canned algorithm that you don't understand; 
3. Use a trick that only requires knowing first-semester undergrad calculus.



### The Hadamard product parametrization of the $L_1$ penalty

#### The math
Here is the extent of the calculus 
you need to know to understand the HPP optimization scheme: 
Let $h(x) =  x + a/x$ for $x>0$ and some fixed $a>0$. 
Here is a plot of this function for $a=2$:

```{r,echo=FALSE,fig.width=5,fig.height=3} 
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
plot(function(x){ x+1/x } ,xlim=c(0,3),ylab="g(x)") 
```

This looks convex. Let's take a derivative and see where 
it is zero: 

$$
\begin{align*}
h'(x)  = 1  - a/x^2  &  = 0  \\  
         x^2 &= a. 
\end{align*}
$$


So the only critical point is $x= \sqrt{a}$. The picture suggests that
this is the function's minimum, but that's not enough for full credit on the 
calculus quiz. Let's take another derivative:

$$
h''(x) = 2a/x^3.
$$

That's positive, so indeed $x=\sqrt{a}$ is the minimum of this function:
\begin{align*}
  \arg \min_{x>0}  x + a/x  &= \sqrt{a} \\ 
   \min_{x>0}  x + a/x  & = 2 \sqrt{a}  \\ 
\end{align*}


#### The reparametrization

Now suppose we have a single (scalar) $\beta$ and want to minimize 
some function $\tilde f(\beta) = f(\beta) + \lambda |\beta|$. 
Here is one way to do it: Write $\beta=uv$ and 
find  values of $u$ and $v$ that minimize the function

\[ \tilde g(u,v) = f(uv) + \lambda u^2/2 + \lambda v^2/2.\]

Let's see why this works:
\begin{align*}
\min_{u,v}  f(uv) + \lambda u^2/2 + \lambda v^2/2   &= 
\min_{\beta,u} f(\beta) + \lambda u^2/2 + \lambda (\beta/u)^2/2 
 \\
 &= \min_\beta f(\beta)  +
    \tfrac{\lambda}{2} \min_u \left [ u^2 + (\beta/u)^2 \right ]   \\
 &=  \min_\beta f(\beta) +
     \tfrac{\lambda}{2} ( 2\sqrt{\beta^2} )  \\
 &=    \min_\beta f(\beta) + \lambda|\beta|.
\end{align*}
The first line follows by letting $\beta= uv$. The third line 
follows from the calculus result above. 

#### The generalization
Now let $\beta$ be a vector, and reparametrize as 
$\beta=u\circ v$ where "$\circ$" is the Hadamard (elementwise)
product of the vectors $u$ and $v$. Applying the same logic as above, 
it follows that 

\[ 
\min_\beta f(\beta) + \lambda \sum |\beta_j|  =
\min_{u,v}  f(uv) + \lambda u^\top u /2 + \lambda v^\top v/2
\]

Why might this be helpful? The function to optimize on the left-hand side 
is convex (if $f$ is) but not differentiable. 
You need to take more math classes if you want to understand how to optimize 
this function directly. 
Alternatively, the 
function to optimize on the 
left hand side is differentiable, and can be optimized by iteratively minimizing
in $u$ and $v$. 


#### Lasso estimates via alternating ridge regressions

Let's return to the $L_1$ penalized linear regression problem:
\[
\hat \beta =  \arg \min_\beta  \beta^\top Q \beta - 2 \beta^\top l  + \lambda ||\beta||_1 
\]
As we discussed above, $\hat \beta = \hat u\circ \hat v$ where 
\[
(\hat u,\hat v) =  \arg \min_{u,v} \    (u\circ v)^\top Q (u\circ v) - 2 (u\circ v )^\top l  +  \tfrac{\lambda}{2}u^\top u + \tfrac{\lambda}{2}v^\top v .
\]
The optimal  $u$ for fixed $v$ is 
\begin{align*} 
 \tilde u & = \arg \min_u  
(u\circ v)^\top Q (u\circ v)-2 (u\circ v)^\top l + \tfrac{\lambda}{2}u^\top u  \\
& = u^\top (Q \circ v v^\top + I  \lambda/2  ) u - 2 u^\top (v\circ l)   \\
 &= (Q \circ v v^\top + I  \lambda/2 )^{-1} (v\circ l). 
\end{align*}
The third line follows from the second by noting that the second line is 
equivalent to the ridge regression criterion where $u$ is the parameter. 
A similar result holds for the optimal value of $v$ given $u$. 
An alternating ridge regression algorithm for finding the lasso 
etimate $\hat \beta = \hat u\circ \hat v$ is therefore to iterate
the following until convergence. 

1. Set $u =(Q \circ v v^\top + I  \lambda/2 )^{-1} (v\circ l)$. 
2. Set $v =(Q \circ u u^\top + I  \lambda/2 )^{-1} (u\circ l)$. 


### Numerical example
Let's try this out with a numerical example - an analysis of some data 
on diabetes progression. We'll hold out the first 100 
observations to use as a test set, with which we will 
evaluate the predictive performance
of the estimators we obtain.
 
```{r}
load(url("http://www2.stat.duke.edu/~pdh10/Datasets/yX_diabetes"))

y<-yX_diabetes[-(1:100),1]  ; ytest<-yX_diabetes[1:100,1]
X<-yX_diabetes[-(1:100),-1] ; Xtest<-yX_diabetes[1:100,-1] 

dim(X) 
colnames(X) 
```

First, the OLS estimates: 

```{r,echo=FALSE}
fit_ols<-lm(y ~ -1 + X) ; beta_ols<-fit_ols$coef
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
plot(sort(beta_ols),col="lightblue",pch=16,xlab="",ylab=expression(hat(beta)))
abline(h=0)
bigc<-which(abs(beta_ols)>.5)
text(rank(beta_ols)[bigc],beta_ols[bigc],colnames(X)[bigc],srt=-45,cex=.7)
```

```{r}
mean( (ytest - Xtest%*%beta_ols)^2 ) 
```

```{r,echo=FALSE}
n<-dim(X)[1] ; p<-dim(X)[2]  

K<-2
s2<-sum(fit_ols$res^2)/(n-p)
t2<-( sum(y^2)-n*s2  )/sum(X^2) 

qpow<-2/K
lambda<-2*s2*( t2*gamma(1/qpow)/gamma(3/qpow) )^(-qpow/2)
```
A lot of the estimated coefficients are close to zero, 
but of course not quite zero. We could just infer that 
these estimated effects are "small", but that doesn't 
sound very sophisticated. Instead, let's
use the Hadamard product parametrization to 
obtain sparse lasso estimates. Here is the code to set up 
the algorithm:
```{r}
Q<-crossprod(X)
l<-crossprod(X,y) 

Il<-diag(ncol(X))*lambda
v<-sqrt(abs(fit_ols$coef))
```
The object `Il` is just a diagonal matrix 
times the penalty parameter $\lambda$. What is 
the value of $\lambda$  and where did it come from? 
The value is `r round(lambda,2)`, and this is 
a moment-based empirical Bayes estimate. 
Here is the optimization algorithm:


```{r}
for(s in 1:100)
{ 
  u<- solve( Q * v%*%t(v) + Il/2 )%*%( l*v )

  v<- solve( Q * u%*%t(u) + Il/2 )%*%( l*u )  
}

beta_l1p<-u*v
```

Here are the resulting lasso estimates. As can be seen, many of the 
estimated coefficients are zero. 
```{r,echo=FALSE}
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
plot(sort(beta_l1p),col="lightblue",pch=16,xlab="",ylab=expression(hat(beta)))
abline(h=0)
bigc<-which(abs(beta_l1p)>1e-5)
text(rank(beta_l1p)[bigc],beta_l1p[bigc],colnames(X)[bigc],srt=-45,cex=.7)
```

The resulting sparse estimate provide slightly improved
predictive performance, as compared to the OLS estimate:

```{r}
mean( (ytest - Xtest%*%beta_l1p)^2 )        
```

Still, there are a lot of coefficient estimates  that are small, but not 
quite zero. Can't we zap these away too? 

We've seen how writing $\beta = u\circ v$ leads to 
penalized (lasso) estimates of $\beta$. Maybe to 
get more penalization we could write $\beta$ as a product of 
more things:

```{r,echo=FALSE}
K<-4
qpow<-2/K
lambda<-2*s2*( t2*gamma(1/qpow)/gamma(3/qpow) )^(-qpow/2)
Il<-diag(ncol(X))*lambda
```


```{r}
v<-w<-x<-(abs(beta_ols))^(.25)   

for(s in 1:100)
{
  u<- solve( Q * (v*w*x)%*%t(v*w*x) + Il/4 )%*%( l*v*w*x )

  v<- solve( Q * (u*w*x)%*%t(u*w*x) + Il/4 )%*%( l*u*w*x )

  w<- solve( Q * (u*v*x)%*%t(u*v*x) + Il/4 )%*%( l*u*v*x )

  x<- solve( Q * (u*v*w)%*%t(u*v*w) + Il/4 )%*%( l*u*v*w )
}

beta_lhp<-u*v*w*x
```


```{r,echo=FALSE}
## -- plot
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0)) 
plot(sort(beta_lhp),col="lightblue",pch=16,xlab="",ylab=expression(hat(beta)))
abline(h=0)
bigc<-which(abs(beta_lhp)>1e-5)
text(rank(beta_lhp)[bigc],beta_lhp[bigc],colnames(X)[bigc],srt=-45,cex=.7) 
## --
```

Indeed, this has led to a "stronger" penalty in the sense that 
now we have mostly coefficients that are zero, and 
a few coefficients 
that are reasonably far from zero. This is very aesthetically pleasing, 
and even better, leads to improved predictive performance:

```{r}
mean( (ytest - Xtest%*%beta_lhp)^2 )
```

What is this crazy estimate, that was obtained by writing 
$\beta$ as the Hadamard product of four quantities? 
In fact, you can show that the resulting estimate 
`beta_lhp` is a local minimizer of the $L_{1/2}$-penalized 
residual sum of squares:

\[
  \hat \beta_{1/2} = 
  \arg \min ||y-X\beta||^2  + \lambda ||\beta||_{1/2}.
\]

This penalty is non-convex, and allows for more shrinkage 
of the small coefficients without biasing the larger coefficients 
as much. 
More on the correspondence 
between this penalty and the Hadamard product parametrization 
can be found in my article 
[arXiv:1611.00040](https://arxiv.org/abs/1611.00040). The article 
also includes 

* a comparison to other optimization methods;
* an algorithm for penalized logistic regression; 
* an new penalty for spatially structured sparsity. 








